{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/grealish/nltk_data'\n    - '/Users/grealish/anaconda3/envs/interview/nltk_data'\n    - '/Users/grealish/anaconda3/envs/interview/share/nltk_data'\n    - '/Users/grealish/anaconda3/envs/interview/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenize\u001b[39;00m \u001b[39mimport\u001b[39;00m word_tokenize\n\u001b[0;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(word_tokenize(\u001b[39m\"\u001b[39;49m\u001b[39mThis is a test sentence.\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
      "File \u001b[0;32m~/anaconda3/envs/interview/lib/python3.11/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    143\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/envs/interview/lib/python3.11/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[39m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[1;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/anaconda3/envs/interview/lib/python3.11/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_punkt_tokenizer\u001b[39m(language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[39mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[0;32m~/anaconda3/envs/interview/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_lang(lang)\n",
      "File \u001b[0;32m~/anaconda3/envs/interview/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_lang\u001b[39m(\u001b[39mself\u001b[39m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[39m=\u001b[39m find(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt_tab/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlang\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1750\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_params \u001b[39m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lang \u001b[39m=\u001b[39m lang\n",
      "File \u001b[0;32m~/anaconda3/envs/interview/lib/python3.11/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/grealish/nltk_data'\n    - '/Users/grealish/anaconda3/envs/interview/nltk_data'\n    - '/Users/grealish/anaconda3/envs/interview/share/nltk_data'\n    - '/Users/grealish/anaconda3/envs/interview/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(\"This is a test sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/grealish/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/grealish/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "data = [\n",
    "    \"What do you do (occupation, role)?\",\n",
    "    \"What type of organization/ company do you work for (industry, size)?\",\n",
    "    \"How long have you been there?\",\n",
    "    \"How do you feel about your job?\",\n",
    "    \"What do you like about it? What would you change?\",\n",
    "    \"If you could rate your job satisfaction on a scale from 1-10, what would you give your current job?\",\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data pre-processing:\n",
    "def pre_process(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [nlp(word)[0].lemma_ for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/grealish/nltk_data'\n    - '/Users/grealish/anaconda3/envs/interview/nltk_data'\n    - '/Users/grealish/anaconda3/envs/interview/share/nltk_data'\n    - '/Users/grealish/anaconda3/envs/interview/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#testing\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sentence \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mWhat do you do? (occupation, role)?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m pre_process(\u001b[39m\"\u001b[39;49m\u001b[39mWhat do you do? (occupation, role)?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m, in \u001b[0;36mpre_process\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      3\u001b[0m text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mlower()\n\u001b[1;32m      4\u001b[0m text \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[^\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms]\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, text)\n\u001b[0;32m----> 5\u001b[0m tokens \u001b[39m=\u001b[39m word_tokenize(text)\n\u001b[1;32m      6\u001b[0m stop_words \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(stopwords\u001b[39m.\u001b[39mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m      7\u001b[0m tokens \u001b[39m=\u001b[39m [word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m tokens \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop_words]\n",
      "File \u001b[0;32m~/anaconda3/envs/interview/lib/python3.11/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    143\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/envs/interview/lib/python3.11/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[39m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[1;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/anaconda3/envs/interview/lib/python3.11/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_punkt_tokenizer\u001b[39m(language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[39mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[0;32m~/anaconda3/envs/interview/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_lang(lang)\n",
      "File \u001b[0;32m~/anaconda3/envs/interview/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_lang\u001b[39m(\u001b[39mself\u001b[39m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[39m=\u001b[39m find(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt_tab/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlang\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1750\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_params \u001b[39m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lang \u001b[39m=\u001b[39m lang\n",
      "File \u001b[0;32m~/anaconda3/envs/interview/lib/python3.11/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/grealish/nltk_data'\n    - '/Users/grealish/anaconda3/envs/interview/nltk_data'\n    - '/Users/grealish/anaconda3/envs/interview/share/nltk_data'\n    - '/Users/grealish/anaconda3/envs/interview/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "sentence = (\"What do you do? (occupation, role)?\")\n",
    "\n",
    "pre_process(\"What do you do? (occupation, role)?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = WordPunctTokenizer()\n",
    "s = PorterStemmer()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What', 'do', 'you', 'do', '?', '(', 'occupation', ',', 'role', ')?']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.tokenize(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'occupation role'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre processing re attempt\n",
    "\n",
    "def process_2(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = t.tokenize(text)\n",
    "    stop = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "process_2(sentence)\n",
    "'''\n",
    "possibly create our own stop words dictionary instead of using given \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "occupation role\n",
      "type organization company work industry size\n",
      "long\n",
      "feel job\n",
      "like would change\n",
      "could rate job satisfaction scale 110 would give current job\n"
     ]
    }
   ],
   "source": [
    "for sent in data:\n",
    "    print(process_2(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "embeddings = model.encode(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of each embedding: (384,)\n",
      "Embedding for the first sentence: [ 3.68108414e-02  3.85988168e-02  5.04045449e-02  1.18477456e-02\n",
      " -4.09782827e-02 -5.49321249e-02  8.49155486e-02 -7.89078176e-02\n",
      " -6.13282397e-02 -2.10117009e-02 -1.42309070e-02 -5.21098971e-02\n",
      " -6.94332793e-02  9.53311697e-02 -7.74688739e-03 -2.79404186e-02\n",
      " -1.55642666e-02  7.18331859e-02 -9.53646097e-03 -3.04570515e-02\n",
      " -6.37162700e-02 -4.29969393e-02  4.22358513e-03 -8.53666570e-03\n",
      " -6.07134635e-03  3.67357060e-02  4.48948368e-02 -4.59118886e-03\n",
      " -4.71928567e-02 -8.88301656e-02 -1.07427891e-02  5.87914698e-02\n",
      "  3.25673148e-02  8.35085437e-02 -6.74968958e-02  1.42367452e-01\n",
      "  7.19204843e-02  6.17467891e-03  6.12898730e-02 -3.03973667e-02\n",
      "  6.35885298e-02 -3.96061353e-02  1.72163229e-02 -4.38369773e-02\n",
      "  4.21521664e-02 -6.39313087e-02  9.56752673e-02 -2.85574086e-02\n",
      "  3.33174951e-02 -2.70078089e-02  1.52374990e-02 -1.48739684e-02\n",
      " -4.92478162e-02  6.20868541e-02  7.60173472e-03  1.92416515e-02\n",
      "  7.44652236e-04 -9.84442979e-03 -7.17975944e-03 -4.51286323e-02\n",
      " -1.42208552e-02  4.03585359e-02 -3.23323160e-02  1.00404449e-01\n",
      " -2.55783158e-03 -4.94617671e-02 -6.20639138e-02  7.27504864e-03\n",
      " -7.01444820e-02 -2.88241077e-02 -1.46006476e-02 -5.38322479e-02\n",
      "  1.03641646e-02  3.03567909e-02  1.33312438e-02 -9.24719498e-02\n",
      "  4.58535664e-02 -9.44167078e-02  3.01318951e-02 -6.59667850e-02\n",
      "  3.97747532e-02 -3.85575071e-02 -3.09141818e-02  7.79207274e-02\n",
      " -5.65928929e-02 -4.15928438e-02  3.77505040e-03  3.65477800e-02\n",
      "  5.23365773e-02 -1.37129880e-03  2.95358623e-04 -4.22968864e-02\n",
      "  3.28432769e-02 -3.03674443e-03 -5.78309484e-02  3.82454246e-02\n",
      "  1.95773542e-02  8.34061280e-02  7.08684837e-03  7.52126873e-02\n",
      " -6.18358655e-03 -2.70522628e-02  2.93196086e-02  4.43736464e-03\n",
      " -9.11921486e-02  3.59713770e-02 -3.71538848e-02  2.88325958e-02\n",
      " -7.76501819e-02 -2.20136251e-02 -2.04973835e-02  4.86174785e-02\n",
      " -6.21418878e-02 -1.15112495e-03  2.62302291e-02  4.72362414e-02\n",
      " -3.09932735e-02  4.96370271e-02 -3.88389379e-02  4.39250395e-02\n",
      "  2.96415798e-02  1.19898684e-01  1.26702851e-02  9.68435854e-02\n",
      " -5.99459261e-02  3.64414081e-02  6.90188929e-02 -7.39872969e-33\n",
      "  2.76698954e-02 -7.87235349e-02  4.01580380e-03  1.47384495e-01\n",
      "  4.86590172e-04  4.21459712e-02  2.57943235e-02  7.39766359e-02\n",
      " -9.85194650e-03  8.09202865e-02  8.74176621e-02  2.80114766e-02\n",
      " -2.93186563e-03  2.63407044e-02 -2.17758361e-02  3.03406846e-02\n",
      " -5.52717112e-02  5.86911365e-02 -6.66179741e-03  9.54215322e-03\n",
      " -1.79125592e-02  6.76378682e-02 -9.49027613e-02  8.44048634e-02\n",
      "  5.19005805e-02 -4.24596779e-02  3.54385898e-02 -9.60441828e-02\n",
      "  4.98458594e-02  4.62102517e-02  1.90178386e-03  4.16046754e-02\n",
      " -5.82986474e-02 -6.91631390e-03 -9.65231098e-03  2.16276553e-02\n",
      " -7.08785430e-02 -2.40931362e-02  8.29334371e-03 -3.52279507e-02\n",
      " -9.72746909e-02 -5.68620563e-02  6.64045243e-03 -1.52222039e-02\n",
      " -2.04554033e-02 -1.13730486e-02  7.01688007e-02  1.31411506e-02\n",
      " -1.61187276e-02  1.39069423e-01 -2.25328468e-02 -4.51424941e-02\n",
      "  4.97080050e-02 -1.99837554e-02 -8.41560028e-03 -2.12322436e-02\n",
      "  4.04805318e-02 -2.88943313e-02 -2.00203173e-02 -8.69948789e-03\n",
      "  6.45900592e-02  5.10117710e-02 -1.14412829e-01  3.58977243e-02\n",
      " -2.04969067e-02 -7.20943135e-05  1.16935829e-02 -3.08172894e-03\n",
      "  1.53534010e-01 -7.01695383e-02 -8.78435150e-02  7.07969069e-02\n",
      " -1.52816065e-03  2.00302582e-02 -6.26427382e-02  3.68465781e-02\n",
      " -2.98574883e-02 -9.25014168e-02 -6.35062233e-02  5.19636236e-02\n",
      "  5.51408827e-02  6.96924608e-03 -3.79632600e-02  4.66153696e-02\n",
      "  4.46919389e-02 -6.16779272e-03  2.77590975e-02 -8.78649205e-02\n",
      "  6.43555000e-02  5.71313798e-02 -8.42360407e-02 -5.10245040e-02\n",
      "  2.75462195e-02  3.46481875e-02 -1.00280186e-02  3.58154615e-33\n",
      "  6.94950372e-02 -5.86480685e-02 -6.24422962e-03 -8.89271274e-02\n",
      " -9.92042292e-03 -9.00555998e-02 -3.53314839e-02 -3.07578407e-02\n",
      " -1.47285657e-02  2.75429711e-02 -7.07934052e-02  8.58739857e-03\n",
      "  3.50229023e-03  2.10265946e-02 -4.34118994e-02 -3.81982587e-02\n",
      " -4.66188230e-02  1.44216130e-02 -3.93956527e-02  4.64433469e-02\n",
      " -7.11364895e-02  4.06888351e-02  4.14364226e-02  5.87205961e-02\n",
      " -4.05780897e-02  1.06781609e-02 -1.75936390e-02  2.57887933e-02\n",
      "  3.78302927e-03  2.22996753e-02  1.40955662e-02  2.40373379e-03\n",
      "  4.45110984e-02 -6.41650036e-02 -6.21896274e-02  1.69504136e-02\n",
      " -2.56435610e-02 -7.45937377e-02  2.92284247e-02 -1.61963683e-02\n",
      "  1.69898011e-02 -6.26637265e-02  3.01865395e-02  2.79175565e-02\n",
      " -1.24402633e-02 -3.38986591e-02 -8.08367040e-04 -3.37604433e-02\n",
      " -5.72574660e-02 -3.89125720e-02 -1.02180173e-03  9.93940700e-03\n",
      " -2.26012096e-02 -8.70497432e-03  5.77547885e-02  7.28237852e-02\n",
      "  6.95248991e-02 -7.89263472e-02 -3.33850235e-02 -1.68932639e-02\n",
      "  4.61947806e-02  5.69207557e-02  5.11374548e-02  7.50495270e-02\n",
      " -3.68518718e-02 -5.67107946e-02 -2.82726344e-02  4.26943600e-02\n",
      " -6.55061379e-02 -5.75707061e-03  2.26647556e-02 -2.52766423e-02\n",
      "  6.15890371e-03 -3.56329158e-02  2.32067937e-03 -1.09893471e-01\n",
      "  3.93653428e-03 -5.65560423e-02 -1.37983607e-02  5.18368371e-02\n",
      " -3.23050804e-02 -9.89237577e-02  6.13763407e-02  1.94315482e-02\n",
      " -7.02517331e-02  8.89895484e-02 -3.30438353e-02  1.53549509e-02\n",
      "  2.67453920e-02 -9.65645984e-02 -2.39252448e-02  5.74839674e-02\n",
      " -2.17639580e-02 -3.40766180e-03  4.68264110e-02 -1.82891355e-08\n",
      " -3.40989716e-02 -8.50716606e-03  6.89641237e-02 -8.27233791e-02\n",
      " -3.53101231e-02 -5.88584878e-02 -4.75665629e-02  2.58663902e-03\n",
      "  4.89791296e-02  3.54109332e-02 -3.63223739e-02 -1.07862055e-01\n",
      "  4.33739573e-02 -3.94388214e-02  2.05301434e-01 -2.45474391e-02\n",
      "  1.02526829e-01  2.68021356e-02 -5.81528358e-02 -6.21945523e-02\n",
      "  7.64284953e-02 -1.70414820e-02 -2.23414339e-02  1.93794817e-02\n",
      " -5.31022325e-02  6.82438724e-03 -5.68842329e-02  1.12026697e-02\n",
      " -4.91174981e-02  1.32600486e-01  4.91048256e-03  5.74763790e-02\n",
      " -9.52757522e-02  5.99843031e-03 -3.80118042e-02 -8.20900351e-02\n",
      "  7.54125714e-02 -9.10552591e-02  8.83072708e-03  1.34162819e-02\n",
      "  6.53549330e-03  1.87432468e-02  4.00686450e-02  3.45859826e-02\n",
      " -1.22453738e-02  1.44399647e-02  4.21460420e-02 -2.93010436e-02\n",
      "  1.09768212e-02 -1.99352968e-02  1.29170977e-02  5.85288042e-04\n",
      " -1.46827530e-02  1.23363854e-02  1.11721521e-02  3.60121951e-02\n",
      "  3.50732431e-02 -3.13932933e-02 -6.72467798e-02  3.55666913e-02\n",
      "  7.38790408e-02 -1.13985334e-02 -7.68321455e-02 -3.29702571e-02]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of each embedding: {embeddings[0].shape}\")\n",
    "\n",
    "print(\"Embedding for the first sentence:\", embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: What do you do (occupation, role)? \n",
      "Cluster: 0\n",
      "\n",
      "Sentence: What type of organization/ company do you work for (industry, size)? \n",
      "Cluster: 0\n",
      "\n",
      "Sentence: How long have you been there? \n",
      "Cluster: 0\n",
      "\n",
      "Sentence: How do you feel about your job? \n",
      "Cluster: 0\n",
      "\n",
      "Sentence: What do you like about it? What would you change? \n",
      "Cluster: 0\n",
      "\n",
      "Sentence: If you could rate your job satisfaction on a scale from 1-10, what would you give your current job? \n",
      "Cluster: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=1, random_state=42)\n",
    "kmeans.fit(embeddings)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"Sentence: {data[i]} \\nCluster: {label}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interview",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
